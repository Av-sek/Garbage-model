# -*- coding: utf-8 -*-
"""Resnet18_`garbage`_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XwUG4G_e72M6vMn5o_Os-ehNRbtTOl0s
"""
from torchvision.datasets import ImageFolder
import datetime
from torch.utils.data import random_split

SRC = "./garbage"

import torch
from torch import nn

# Tier-2 ResBlock class. (Resnet-18,Resnet-34)


class ResBlock_Tier2(nn.Module):
    def __init__(self, in_channels, intermediate_channels):
        super(ResBlock_Tier2, self).__init__()

        # Layers
        # To make the residual input equal size as output channel.

        downsample = False
        self.skip_connection = nn.Sequential()  # Default

        if intermediate_channels == 2 * in_channels:
            self.skip_connection = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    intermediate_channels,
                    kernel_size=1,
                    stride=2,
                    bias=False,
                ),
                nn.BatchNorm2d(intermediate_channels),
            )
            downsample = True

        # Downsampling the output shape.
        if downsample:
            self.conv1 = nn.Conv2d(
                in_channels,
                intermediate_channels,
                kernel_size=3,
                stride=2,
                padding=1,
                bias=False,
            )

        else:
            self.conv1 = nn.Conv2d(
                in_channels,
                intermediate_channels,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
            )

        self.bn1 = nn.BatchNorm2d(intermediate_channels)

        self.conv2 = nn.Conv2d(
            intermediate_channels,
            intermediate_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
        )
        self.bn2 = nn.BatchNorm2d(intermediate_channels)

        self.relu = nn.ReLU()

    def forward(self, x):
        # Residual to be added later.
        identity = self.skip_connection(x)

        # ---------------------------
        # Layer-1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        # ----------------------------

        # ---------------------------
        # Layer-2
        x = self.conv2(x)
        x = self.bn2(x)
        # print(x.size())
        # print(identity.size())
        x += identity
        x = self.relu(x)
        # ----------------------------

        return x


# ---------------------------------------------------------------------------------------------------
    """
    This is Tier-2 Resnet class (Resnet-18, 34)
    It takes input as img_channels:
    for RGB = 3
    for Gray = 1),

    num_layers:
    for Resnet-18 = [2, 2, 2, 2]
    for Resnet-34 = [3, 4, 6, 3]

    , and number of classes.
    for ex : Imagenet = 1000, MNIST = 10 etc

    Output : Resnet Model

    """


class ResNet_Tier2(nn.Module):
    def __init__(self, img_channels, num_layers, num_classes):
        super(ResNet_Tier2, self).__init__()

        # Layers
        # Layer-0 Output shape : 64 X 56 X 56
        self.layer0 = nn.Sequential(
            nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Residual Blocks layers
        self.layer1 = self._make_layer(ResBlock_Tier2, num_layers[0], 64, 64)
        self.layer2 = self._make_layer(ResBlock_Tier2, num_layers[1], 64, 128)
        self.layer3 = self._make_layer(ResBlock_Tier2, num_layers[2], 128, 256)
        self.layer4 = self._make_layer(ResBlock_Tier2, num_layers[3], 256, 512)

        # FC Layers
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.reshape(x.shape[0], -1)
        x = self.fc(x)

        return x

    def _make_layer(
        self, ResBlock_Tier2, num_residual_blocks, in_channels, intermediate_channels
    ):
        layers = []
        only_once = True

        for i in range(num_residual_blocks):
            layers.append(ResBlock_Tier2(in_channels, intermediate_channels))
            if only_once:
                in_channels = intermediate_channels
                only_once = False

        return nn.Sequential(*layers)


# Tier-2 Resnets.
def ResNet18(img_channel=3, num_classes=1000):
    return ResNet_Tier2(img_channel, [2,2,2,2], num_classes)


# Import useful modules.
import os, random
import numpy as np
import pandas as pd
import cv2
from matplotlib import pyplot as plt

os.listdir(SRC)

DATA_DISTRIBUTION = {}

for file in os.listdir(SRC):
    DATA_DISTRIBUTION[f"{file}"] = len(os.listdir(os.path.join(SRC, file)))

# Total Data Distribution.
print(DATA_DISTRIBUTION)

# Data Distribution
fig, ax = plt.subplots()
bars = ax.barh(list(DATA_DISTRIBUTION.keys()), list(DATA_DISTRIBUTION.values()))

ax.bar_label(bars)

# Randomly Visualizing the samples.
imgs = []
classes = []
for file in os.listdir(SRC):
    temp = os.path.join(SRC, file)
    sample = random.choice(os.listdir(temp))
    img = cv2.imread(os.path.join(temp, sample))
    imgs.append(img)
    classes.append(file)

f, ax = plt.subplots(3, 2, figsize=(12, 12))
plt.savefig("random_samples.png")

cnt = 0
for i in range(3):
    ax[i, 0].imshow(imgs[cnt])
    ax[i, 0].set_title(classes[cnt])
    ax[i, 1].imshow(imgs[cnt + 1])
    ax[i, 1].set_title(classes[cnt + 1])
    cnt += 2

# Data Preparation.


import torch
from torch.utils.data import Dataset
from PIL import Image
import re


# Datset Class

# ----------------------------------------------------------------------------------------------------

import torchvision


def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.figure(figsize=(12, 12))
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.savefig(datetime.datetime.now().strftime("%Y%m%d-%H%M%S") + ".png")
    plt.pause(0.001)  # pause a bit so that plots are updated


# -------------------------------------------------------------------------------------------------------


# Model Performance on test data
def calculate_loss_and_accuracy(model, dataloader, size_of_dataset, criterion):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Now set model to validation mode.
    running_loss = 0
    running_accuracy = 0

    # Processing the Test Loader
    for inputs, labels in dataloader:
        # Load data to device.
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Outputs
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)

        # Loss and Backpropagation.
        loss = criterion(outputs, labels)

        # Statistics
        running_loss += loss.item() * inputs.size(0)
        running_accuracy += torch.sum(preds == labels.data)

    epoch_loss = running_loss / size_of_dataset
    epoch_accuracy = running_accuracy / size_of_dataset

    return epoch_loss, epoch_accuracy


# ------------------------------------------------------------------------------------------------
import copy


import matplotlib.pyplot as plt

def train(model, criterion, optimizer, scheduler, num_of_epochs):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    track_training_loss = []  # Track training loss for plotting
    track_training_accuracy = []  # Track training accuracy for plotting
    track_val_loss = []  # Track validation loss for plotting
    track_val_accuracy = []  # Track validation accuracy for plotting

    for epoch in range(num_of_epochs):
        print(f"\nEpoch {epoch + 1}/{num_of_epochs}")
        print("-" * 30)

        model.train()  # Setting model to train.
        running_loss = 0
        running_accuracy = 0

        # Processing the Train Loader
        for inputs, labels in train_loader:
            # Load data to device.
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()  # zero the parameter gradients

            # Outputs
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            # Loss and Backpropagation.
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # Statistics
            running_loss += loss.item() * inputs.size(0)
            running_accuracy += torch.sum(preds == labels.data)

        scheduler.step()
        epoch_loss = running_loss / len(train_dataset)
        epoch_accuracy = running_accuracy / len(train_dataset)
        track_training_loss.append(epoch_loss)  # Loss Tracking
        track_training_accuracy.append(epoch_accuracy)  # Accuracy Tracking

        print(f"Training Loss: {epoch_loss:.4f} Training Acc.: {epoch_accuracy:.4f}")

        # Now set model to validation mode.
        model.eval()
        with torch.no_grad():
            val_loss, val_accuracy = calculate_loss_and_accuracy(
                model, val_loader, len(val_dataset), criterion
            )

            track_val_loss.append(val_loss)  # Loss Tracking
            track_val_accuracy.append(val_accuracy)  # Accuracy Tracking
            print(f"Val Loss: {val_loss:.4f} Val Acc.: {val_accuracy:.4f}\n")
            if val_accuracy > best_acc:
                print("Found better model...")
                print("Updating the model weights....\n")

                best_acc = val_accuracy
                best_model_wts = copy.deepcopy(model.state_dict())

    model.load_state_dict(best_model_wts)  # update model
    # Plotting
    epochs_range = range(1, num_of_epochs + 1)

    plt.figure(figsize=(12, 6))

    # Plot training loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, track_training_loss, label="Training Loss")
    plt.plot(epochs_range, track_val_loss, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training and Validation Loss")

    # Plot training accuracy
    plt.subplot(1, 2, 2)
    # Move the tensors to the CPU before plotting
    plt.plot(epochs_range, [acc.cpu().numpy() for acc in track_training_accuracy], label="Training Accuracy")
    plt.plot(epochs_range, [acc.cpu().numpy() for acc in track_val_accuracy], label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title("Training and Validation Accuracy")

    plt.tight_layout()
    plt.savefig("accuracy.png")


    return model

# Hyper-params
BATCH_SIZE = 32

# Data Prepration.
from torchvision import transforms
from torch.utils.data import DataLoader

# Just normalization for validation
data_transforms = {
    "train": transforms.Compose(
        [
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
        ]
    ),
    "val": transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
        ]
    ),
}

# Load Dataset.
transformations = transforms.Compose(
    [transforms.Resize((256, 256)), transforms.ToTensor()]
)

dataset = ImageFolder(SRC, transform=data_transforms["train"])
print(dataset.classes)
total_size = len(dataset)
train_split = int(0.7 * total_size)
val_split = int(0.2 * total_size)
test_split = total_size - train_split - val_split

train_dataset, val_dataset, test_dataset = random_split(
    dataset, [train_split, val_split, test_split]
)
# Datloaders
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)

class_names = ["glass", "paper", "cardboard", "plastic", "metal", "trash"]
data_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)
inputs, classes = next(iter(data_loader))
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

# imshow(out, title=[class_names[x] for x in classes])

# Pytorch Standard Resnet-18 Model.
from torchvision import models
from torch.optim import lr_scheduler
import torch.optim as optim

NUM_OF_EPOCHS = 100

# Our Resnet Model
model = ResNet18(img_channel=3, num_classes=len(dataset.classes))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Criterion.
criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

# Training
best_model = train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=exp_lr_scheduler,
    num_of_epochs=NUM_OF_EPOCHS,
)

torch.save(model.state_dict(), "new_model.pth")
